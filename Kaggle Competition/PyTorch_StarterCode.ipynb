{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Starter Code\n",
    "Use this code as a template, starting place, or inspiration... whatever helps you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This starter code will be using the following packages:\n",
    "- `torch`\n",
    "- `torchtext`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `tqdm`\n",
    "- `matplotlib`\n",
    "\n",
    "Be sure to install these using either `pip` or `conda`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Standard Library\n",
    "import math\n",
    "import os\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "# PyTorch Modules\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "\n",
    "# Other Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "Visit [https://www.kaggle.com/competitions/osuaiclub-fall2022-sentiment-analysis/data](https://www.kaggle.com/competitions/osuaiclub-fall2022-sentiment-analysis/data) to download the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will be using the `pandas` package to load in our data. All the data is conveniently stored in a `.csv` file which is really easy to construct a `pandas` dataframe out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR, 'train.csv')):\n",
    "    train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), index_col='id')\n",
    "else:\n",
    "    train_df = pd.read_csv(\"https://raw.githubusercontent.com/OSU-AIClub/Fall-2022/main/Kaggle%20Competition/data/train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Subset of Dataset for Quicker Experimentation\n",
    "We recommend using and triaining on a small subset of the dataset while you are prototyping and trying to get your model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the size of the dataset\n",
    "num_samples = len(train_df.index)\n",
    "\n",
    "# Define how many samples we want in our smaller dataset\n",
    "target_num_samples = 1000\n",
    "\n",
    "# Calculate how many training samples we need to remove\n",
    "n_remove = num_samples - target_num_samples\n",
    "\n",
    "# Randomly choose the n_remove indices we will remove\n",
    "drop_indices = np.random.choice(train_df.index, n_remove, replace=False)\n",
    "train_df = train_df.drop(drop_indices)\n",
    "\n",
    "# Show the remaining dataframe\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Class Imbalance in Dataset\n",
    "This dataset heavily favors the `1` sentiment, which represents a positive sentiment. This results in there being relatively more positive training samples than there are negative training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will address this imbalance with [undersampling](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/) by reducing the number of positive sentiment samples in the dataset at random until it matches the number of negative sentiment samples. While we do not have a significant class imbalance, this can slightly help. Try removing the undersampling and see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define values for positive and negative sentiment\n",
    "POSITIVE_SENTIMENT = 1\n",
    "NEGATIVE_SENTIMENT = 0\n",
    "\n",
    "# Count the number of positive and negative samples\n",
    "num_pos_samples = train_df['sentiment'].value_counts()[POSITIVE_SENTIMENT] \n",
    "num_neg_samples = train_df['sentiment'].value_counts()[NEGATIVE_SENTIMENT]\n",
    "\n",
    "# Calculate the number of positive samples we need to remove to have \n",
    "# the same number as negative samples \n",
    "num_pos_remove = max(num_pos_samples - num_neg_samples,0)\n",
    "num_pos_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Dataset into Dataframes of Postive and Negative Only Samples\n",
    "pos_df = train_df[train_df['sentiment'] == POSITIVE_SENTIMENT]\n",
    "neg_df = train_df[train_df['sentiment'] == NEGATIVE_SENTIMENT]\n",
    "print(pos_df.head(), neg_df.head())\n",
    "# Randomly caluclate the postive dataframe indeces to remove\n",
    "pos_drop_indices = np.random.choice(pos_df.index, num_pos_remove, replace=False)\n",
    "\n",
    "# Drop Selected Samples from the Positive Dataframe to balance out both sentiment values\n",
    "pos_undersampled = pos_df.drop(pos_drop_indices)\n",
    "pos_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the negative samples and the positive samples into one dataframe\n",
    "balanced_train_df = pd.concat([neg_df, pos_undersampled])\n",
    "\n",
    "# Check the counts to make sure the classes are now even\n",
    "balanced_train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_train_df.to_csv(os.path.join(DATA_DIR, 'all_sets.csv'))\n",
    "\n",
    "TOTAL_SAMPLES = balanced_train_df.shape[0]\n",
    "TOTAL_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Now that we have created the training and testing split for our data, we can use techniques like tokenization to make the dataset easier for our model to process and train on. We will only be showing how to apply tokenization, but we encourage you to try other techniques!\n",
    "\n",
    "We will be using the PyTorch torchtext libary to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a \"Vocabulary\"\n",
    "Next, we need to create a \"vocabulary\" of all words in the dataset. In NLP, a vocabulary is the mapping of each word to a unique ID. We will represent words in numerical form for the model to be able to interpret them.\n",
    "\n",
    "By creating this mapping, one can write a sentence with numbers. For instance, if the vocab is as follows:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"i\": 0,\n",
    " \"the\": 1,\n",
    " \"ate\": 2,\n",
    " \"pizza\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "We can say \"I ate the pizza\" by saying `[0, 2, 1, 3]`.\n",
    "\n",
    "This is an oversimplified explanation of encoding, but the general idea is the same.\n",
    "\n",
    "\n",
    "`<START>` and `<END>` represent the start and end of the sample respectively. They are tokens used to identify the beginning and ending of each sentence in order to train the model. As shown, they will be inserted at the beginning and end of each sample.\n",
    "\n",
    "`<UNK>` is the token used to represent any word not in our vocabulary. This is most useful when you want to limit the vocabulary size to increase the speed of training or run inference on text never seen before. \n",
    "\n",
    "`<PAD>` is the token used to pad shorter inputs to the length of the longest input, to ensure we can have a constant input size for batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data Processing Pipelines\n",
    "The first step is to load the raw data from the `.csv` file, and then define our Tokenizer and Vocab(ulary). Using these, we can define piplines which will take any input text and convert it to a list of numeric tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "for (_,text, sentiment) in list(pd.read_csv(os.path.join(DATA_DIR,'all_sets.csv')).itertuples(index=False, name=None)):\n",
    "    tokenized = tokenizer(text)\n",
    "    counter.update(tokenized)\n",
    "    \n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(counter, min_freq = 10, specials=('<UNK>', '<START>', '<END>', '<PAD>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how we convert a token to a number in the vocab\n",
    "def vocab_token(token):\n",
    "    if token in vocab:\n",
    "        return vocab[token]\n",
    "    else:\n",
    "        return vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definte how we process the input text from the CSV file for use in the Torch Dataset\n",
    "def text_pipeline(x):\n",
    "    return [vocab_token(token) for token in tokenizer(x)]\n",
    "    \n",
    "# Definte how we process the input label from the CSV file for use in the Torch Dataset\n",
    "def label_pipline(y):\n",
    "    return int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use of the text_pipeline\n",
    "text_pipeline('This is an incredibly amazing absolutely perfect example.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Object\n",
    "The next step is to define the PyTorch `Dataset` object we will use to load the data. You can find more information on PyTorch Datasets [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token numbers \n",
    "PAD_IDX = vocab['<PAD>']\n",
    "START_IDX = vocab['<START>']\n",
    "END_IDX = vocab['<END>']\n",
    "\n",
    "# Define whether we are laoding this data in the CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Dataset Class\n",
    "class SentimentDataset(Dataset):\n",
    "    \n",
    "    # Helper function\n",
    "    def process_input(self, sample):\n",
    "        # unpack sample input\n",
    "        _, _text, _label = sample\n",
    "        \n",
    "        # Pass input through their respective pipelines and construct a tensor out of them\n",
    "        label = torch.tensor([label_pipline(_label)], dtype=torch.int32)\n",
    "        text = torch.tensor([START_IDX] + text_pipeline(_text) + [END_IDX], dtype=torch.int32)\n",
    "        \n",
    "        # Return the tesnor input\n",
    "        return text, label\n",
    "    \n",
    "    def __init__(self, csv_file_path):\n",
    "        # Load the raw data from file\n",
    "        self.data = list(pd.read_csv(csv_file_path).itertuples(index=False, name=None))\n",
    "        \n",
    "        # Process the data and pass through data pipelines; convert to tensors\n",
    "        self.processed_data = [self.process_input(x) for x in self.data]\n",
    "        \n",
    "        # Pad and Separate Text Input Lengths\n",
    "        self.inpts = pad_sequence([x for x, _ in self.processed_data], batch_first=True, padding_value=PAD_IDX)\n",
    "        \n",
    "        # Separate Labels from Text\n",
    "        self.labels = [y for _, y in self.processed_data]\n",
    "        \n",
    "    # Required function for Dataset subclasses\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Called whenever a data sample is generated\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inpts[idx].to(device), self.labels[idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Dataset Object\n",
    "dataset = SentimentDataset(os.path.join(DATA_DIR, 'all_sets.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DataLoader Object\n",
    "Next, we need to create the `DataLoader` object, which is used to concatonate samples into iterable batches. Here we need to define the batch size, the ratio of the training split to validation split (we use the validation dataset for an unbiased performance metric - see more information about hold-out datasets [here](https://www.datarobot.com/wiki/training-validation-holdout/)) and then generate the training data loaders and validation dataloaders respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.15\n",
    "\n",
    "# Calculate the number of samples in the training dataset\n",
    "train_counts = math.ceil(len(dataset) * train_ratio)\n",
    "\n",
    "# Calculate the number of samples in the validation dataset\n",
    "val_counts = len(dataset) - train_counts\n",
    "\n",
    "print(train_counts, val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Split the dataset into a Training Split and Validation Split\n",
    "train_ds, val_ds = random_split(dataset, [train_counts, val_counts])\n",
    "\n",
    "# Create Training and Validation DataLoaders\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Now that we have our data, we can definte our model - the final step before we train it. This is the most important part of the code! I recommend trying out different models, changing architectures, etc. to see what works best! This model is not good - it only achieves at best 50% accuracy. However, that is part of the fun! It is on you to see how you can design and train a model to do even better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, n_layers, vocab_size, output_dim, hidden_dim, embedding_dim, drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        # Define Fully Connected Layer Hyperparameters\n",
    "        self.output_dim = output_dim\n",
    "        self.drop_prob = drop_prob\n",
    " \n",
    "        # Define LSTM and Embedding Hyperparameters\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "        # Define Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=self.n_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # Dropout Layer - Reduces Overfitting\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "    \n",
    "        # Linear (Fully Connected) and Sigmoid Activation\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    \n",
    "    # Required function for PyTorch Models\n",
    "    #       Defines how inputs are passed layer by layer\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize Hidden State\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch_first = True\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.n_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        \n",
    "        hidden = (h0.data, c0.data)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Hyperparameters\n",
    "n_layers = 2\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 400\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "# Instantiate Model\n",
    "model = SentimentRNN(n_layers, vocab_size, output_dim, hidden_dim, embedding_dim,drop_prob=0.5)\n",
    "\n",
    "# Move model to GPU\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "The final step is to train the model. I recommend playing around with the training hyperparameters to see what achieves the best result. Additionally, note that if you change the model class, it might mean you have to change slight things in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Hyperparameters\n",
    "lr=0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "clip = 5\n",
    "epochs = 100\n",
    "\n",
    "# Accuracy Metric\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for keeping track of training history\n",
    "valid_loss_min = np.Inf\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "# Load Previously Trained Model if Exists\n",
    "if os.path.exists('./models/state_dict.pt'):\n",
    "    print(\"Loading existing model...\")\n",
    "    # model.load_state_dict(torch.load('./models/state_dict.pt'))\n",
    "    #       Note that the above line throws an error when you change hyperparameters\n",
    "    #       For that reeason, it is commented out for now.\n",
    "\n",
    "for epoch in (range(epochs)):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in tqdm(train_dl):\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        \n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "\n",
    "    # Evaluate on Validation Dataset\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in tqdm(val_dl):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float().squeeze())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "       \n",
    "    # Calculate and Save Training Statistics     \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_dl.dataset)\n",
    "    epoch_val_acc = val_acc/len(val_dl.dataset)\n",
    "\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    \n",
    "    # Print Training Metrics\n",
    "    print(f'Epoch {epoch+1}/{epochs}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    \n",
    "    # Save Model if this is the best model yet\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './models/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    \n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training/Validaiton Accuracy and Loss Per Epoch\n",
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference On Test Dataset\n",
    "We have now fully trained our model! The next task is to perform inference on the test dataset so we can save our predictions and submit them to Kaggle. The first step is to load the best model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model with the same parameters\n",
    "inference_model = SentimentRNN(n_layers, vocab_size, output_dim, hidden_dim, embedding_dim, drop_prob=0.5)\n",
    "\n",
    "# Load the saved weights and biases from file\n",
    "inference_model.load_state_dict(torch.load('./models/state_dict.pt'))\n",
    "\n",
    "# Prep model for inference\n",
    "inference_model.eval()\n",
    "inference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text processing and model output interpretting\n",
    "def inference(model, text):\n",
    "    # Process input sentence\n",
    "    text_tensor = torch.tensor([START_IDX] + text_pipeline(text) + [END_IDX], dtype=torch.int32).to(device).unsqueeze(0)\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    batch_size = 1\n",
    "    h = model.init_hidden(batch_size)\n",
    "    h = tuple([x.data for x in h])\n",
    "    \n",
    "    # Pass through model\n",
    "    output, h = model(text_tensor, h)\n",
    "    prediction = output.item()\n",
    "    \n",
    "    # Return prediction\n",
    "    return 1 if prediction > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download test dataset if it doesn't exist locally\n",
    "if os.path.exists(os.path.join(DATA_DIR, 'test.csv')):\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), index_col='id')\n",
    "else:\n",
    "    test_df = pd.read_csv(\"https://raw.githubusercontent.com/OSU-AIClub/Fall-2022/main/Kaggle%20Competition/data/test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Inference on Test Dataset \n",
    "predictions = [] # {id,prediction}\n",
    "\n",
    "for id_, text in tqdm(list(test_df).itertuples(index=True, name=None)):\n",
    "    prediction = inference(model, text)\n",
    "    predictions.append({'id': id_, 'sentiment': prediction})\n",
    "\n",
    "\n",
    "# Save to CSV file\n",
    "preds = pd.DataFrame(predictions)\n",
    "preds.to_csv('submission.csv', index=False)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Submit!\n",
    "We now are ready to submit our predictions. Uplodad your `submission.csv` file to do this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fc8f51b3e16566cc87b17e2b645c7bac967f770095d25cf6c3af0b0fb8cea7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
