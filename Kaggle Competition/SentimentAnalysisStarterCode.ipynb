{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Starter Code\n",
    "Use this code as a template, starting place, or inspiration... whatever helps you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This starter code will be using the following packages:\n",
    "- `Pandas`\n",
    "- `NumPy`\n",
    "- `PyTorch`\n",
    "- `nltk`\n",
    "Be sure to install these using either `pip` or `conda`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "Visit [https://www.kaggle.com/competitions/osuaiclub-fall2022-nlp-challenge/data](https://www.kaggle.com/competitions/osuaiclub-fall2022-nlp-challenge/data) to download the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will be using the `pandas` package to load in our data. All the data is conveniently stored in a `.csv` file which is really easy to construct a `pandas` dataframe out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>\" While sporadically engrossing (including a f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>A French novelist, disgusted by his wife's soc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>It must be remembered that the Gammera movies,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>I gave this film 8 out of 10, reserving 10 for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>Cheap, gloriously bad cheese from the 80's, th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "id                                                                 \n",
       "10000  \" While sporadically engrossing (including a f...          0\n",
       "10001  A French novelist, disgusted by his wife's soc...          1\n",
       "10002  It must be remembered that the Gammera movies,...          0\n",
       "10003  I gave this film 8 out of 10, reserving 10 for...          1\n",
       "10004  Cheap, gloriously bad cheese from the 80's, th...          0\n",
       "...                                                  ...        ...\n",
       "49995  I thought this movie did a down right good job...          1\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...          0\n",
       "49997  I am a Catholic taught in parochial elementary...          0\n",
       "49998  I'm going to have to disagree with the previou...          0\n",
       "49999  No one expects the Star Trek movies to be high...          0\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), index_col='id')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Subset of Dataset for Quicker Experimentation\n",
    "We recommend using and triaining on a small subset of the dataset while you are prototyping and trying to get your model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10025</th>\n",
       "      <td>Recipe for one of the worst movies of all time...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>Ughh this movie is awful. The script is stupid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10160</th>\n",
       "      <td>i saw this movie when i was 13 and i really li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>the first Child's Play was an original and eff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10184</th>\n",
       "      <td>This is one of those films the British Lottery...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49788</th>\n",
       "      <td>OK, I would not normally watch a Farrelly brot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49791</th>\n",
       "      <td>The college teacher Larry Donner (Billy Crista...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49884</th>\n",
       "      <td>This Gundam series only follows Gundam 0083 St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49948</th>\n",
       "      <td>It is the early morning of our discontent, and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49964</th>\n",
       "      <td>I saw this last week during Bruce Campbell's b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "id                                                                 \n",
       "10025  Recipe for one of the worst movies of all time...          0\n",
       "10071  Ughh this movie is awful. The script is stupid...          0\n",
       "10160  i saw this movie when i was 13 and i really li...          1\n",
       "10164  the first Child's Play was an original and eff...          1\n",
       "10184  This is one of those films the British Lottery...          0\n",
       "...                                                  ...        ...\n",
       "49788  OK, I would not normally watch a Farrelly brot...          1\n",
       "49791  The college teacher Larry Donner (Billy Crista...          1\n",
       "49884  This Gundam series only follows Gundam 0083 St...          1\n",
       "49948  It is the early morning of our discontent, and...          0\n",
       "49964  I saw this last week during Bruce Campbell's b...          1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the size of the dataset\n",
    "num_samples = len(train_df.index)\n",
    "\n",
    "# Define how many samples we want in our smaller dataset\n",
    "target_num_samples = 1000\n",
    "\n",
    "# Calculate how many training samples we need to remove\n",
    "n_remove = num_samples - target_num_samples\n",
    "\n",
    "# Randomly choose the n_remove indices we will remove\n",
    "drop_indices = np.random.choice(train_df.index, n_remove, replace=False)\n",
    "train_df = train_df.drop(drop_indices)\n",
    "\n",
    "# Show the remaining dataframe\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Class Imbalance in Dataset\n",
    "This dataset heavily favors the `1` sentiment, which represents a positive sentiment. This results in there being significantly more positive training samples than there are negative training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    516\n",
       "1    484\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will address this imbalance with [undersampling](https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/) by reducing the number of positive sentiment samples in the dataset at random until it matches the number of negative sentiment samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define values for positive and negative sentiment\n",
    "POSITIVE_SENTIMENT = 1\n",
    "NEGATIVE_SENTIMENT = 0\n",
    "\n",
    "# Count the number of positive and negative samples\n",
    "num_pos_samples = train_df['sentiment'].value_counts()[POSITIVE_SENTIMENT] \n",
    "num_neg_samples = train_df['sentiment'].value_counts()[NEGATIVE_SENTIMENT]\n",
    "\n",
    "# Calculate the number of positive samples we need to remove to have \n",
    "# the same number as negative samples \n",
    "num_pos_remove = max(num_pos_samples - num_neg_samples,0)\n",
    "num_pos_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review  sentiment\n",
      "id                                                                 \n",
      "10160  i saw this movie when i was 13 and i really li...          1\n",
      "10164  the first Child's Play was an original and eff...          1\n",
      "10318  I haven't liked many TV shows post 1990, but T...          1\n",
      "10348  Often considered Argento's last \"great\" film, ...          1\n",
      "10440  After the success of the second instalment, Ri...          1                                                   review  sentiment\n",
      "id                                                                 \n",
      "10025  Recipe for one of the worst movies of all time...          0\n",
      "10071  Ughh this movie is awful. The script is stupid...          0\n",
      "10184  This is one of those films the British Lottery...          0\n",
      "10408  Given this film's incredible reviews I was exp...          0\n",
      "10493  One question: Why? First off, the premise is n...          0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10160</th>\n",
       "      <td>i saw this movie when i was 13 and i really li...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10164</th>\n",
       "      <td>the first Child's Play was an original and eff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10318</th>\n",
       "      <td>I haven't liked many TV shows post 1990, but T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10348</th>\n",
       "      <td>Often considered Argento's last \"great\" film, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10440</th>\n",
       "      <td>After the success of the second instalment, Ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49716</th>\n",
       "      <td>I really enjoy this movie. The first time it w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49788</th>\n",
       "      <td>OK, I would not normally watch a Farrelly brot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49791</th>\n",
       "      <td>The college teacher Larry Donner (Billy Crista...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49884</th>\n",
       "      <td>This Gundam series only follows Gundam 0083 St...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49964</th>\n",
       "      <td>I saw this last week during Bruce Campbell's b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>484 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "id                                                                 \n",
       "10160  i saw this movie when i was 13 and i really li...          1\n",
       "10164  the first Child's Play was an original and eff...          1\n",
       "10318  I haven't liked many TV shows post 1990, but T...          1\n",
       "10348  Often considered Argento's last \"great\" film, ...          1\n",
       "10440  After the success of the second instalment, Ri...          1\n",
       "...                                                  ...        ...\n",
       "49716  I really enjoy this movie. The first time it w...          1\n",
       "49788  OK, I would not normally watch a Farrelly brot...          1\n",
       "49791  The college teacher Larry Donner (Billy Crista...          1\n",
       "49884  This Gundam series only follows Gundam 0083 St...          1\n",
       "49964  I saw this last week during Bruce Campbell's b...          1\n",
       "\n",
       "[484 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the Dataset into Dataframes of Postive and Negative Only Samples\n",
    "pos_df = train_df[train_df['sentiment'] == POSITIVE_SENTIMENT]\n",
    "neg_df = train_df[train_df['sentiment'] == NEGATIVE_SENTIMENT]\n",
    "print(pos_df.head(), neg_df.head())\n",
    "# Randomly caluclate the postive dataframe indeces to remove\n",
    "pos_drop_indices = np.random.choice(pos_df.index, num_pos_remove, replace=False)\n",
    "\n",
    "# Drop Selected Samples from the Positive Dataframe to balance out both sentiment values\n",
    "pos_undersampled = pos_df.drop(pos_drop_indices)\n",
    "pos_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    516\n",
       "1    484\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the negative samples and the positive samples into one dataframe\n",
    "balanced_train_df = pd.concat([neg_df, pos_undersampled])\n",
    "\n",
    "# Check the counts to make sure the classes are now even\n",
    "balanced_train_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_train_df.to_csv(os.path.join(DATA_DIR, 'all_sets.csv'))\n",
    "\n",
    "TOTAL_SAMPLES = balanced_train_df.shape[0]\n",
    "TOTAL_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Now that we have created the training and testing split for our data, we can use techniques like tokenization to make the dataset easier for our model to process and train on. We will only be showing how to apply tokenization, but we encourage you to try other techniques!\n",
    "\n",
    "We will be using the PyTorch torchtext libary to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a \"Vocabulary\"\n",
    "Next, we need to create a \"vocabulary\" of all words in the dataset. In NLP, a vocabulary is the mapping of each word to a unique ID. We will represent words in numerical form for the model to be able to interpret them.\n",
    "\n",
    "By creating this mapping, one can write a sentence with numbers. For instance, if the vocab is as follows:\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"i\": 0,\n",
    " \"the\": 1,\n",
    " \"ate\": 2,\n",
    " \"pizza\": 3\n",
    "}\n",
    "```\n",
    "\n",
    "We can say \"I ate the pizza\" by saying `[0, 2, 1, 3]`.\n",
    "\n",
    "This is an oversimplified explanation of encoding, but the general idea is the same.\n",
    "\n",
    "\n",
    "`<START>` and `<END>` represent the start and end of the sample respectively. They are tokens used to identify the beginning and ending of each sentence in order to train the model. As shown, they will be inserted at the beginning and end of each sample.\n",
    "\n",
    "`<UNK>` is the token used to represent any word not in our vocabulary. This is most useful when you want to limit the vocabulary size to increase the speed of training or run inference on text never seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Data Processing Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "for (_,text, sentiment) in list(pd.read_csv(os.path.join(DATA_DIR,'all_sets.csv')).itertuples(index=False, name=None)):\n",
    "    tokenized = tokenizer(text)\n",
    "    counter.update(tokenized)\n",
    "    \n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = vocab(counter, min_freq = 10, specials=('<UNK>', '<START>', '<END>', '<PAD>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_token(token):\n",
    "    if token in vocab:\n",
    "        return vocab[token]\n",
    "    else:\n",
    "        return vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 64, 111]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab_token(token) for token in \"this is an\".split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return [vocab_token(token) for token in tokenizer(x)]\n",
    "    \n",
    "    \n",
    "def label_pipline(y):\n",
    "    return int(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[379, 64, 111, 1211]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is an example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_IDX = vocab['<PAD>']\n",
    "START_IDX = vocab['<START>']\n",
    "END_IDX = vocab['<END>']\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \n",
    "    def process_input(self, sample):\n",
    "        _, _text, _label = sample\n",
    "        label = torch.tensor([label_pipline(_label)], dtype=torch.int32)\n",
    "        text = torch.tensor([START_IDX] + text_pipeline(_text) + [END_IDX], dtype=torch.int32)\n",
    "        return text, label\n",
    "    \n",
    "    def __init__(self, csv_file_path):\n",
    "        self.data = list(pd.read_csv(csv_file_path).itertuples(index=False, name=None))\n",
    "        self.processed_data = [self.process_input(x) for x in self.data]\n",
    "        self.inpts = pad_sequence([x for x, _ in self.processed_data], batch_first=True, padding_value=PAD_IDX)\n",
    "        self.labels = [y for _, y in self.processed_data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inpts[idx].to(device), self.labels[idx].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentimentDataset(os.path.join(DATA_DIR, 'all_sets.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DataLoader Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "850 150\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_counts = math.ceil(len(dataset) * train_ratio)\n",
    "val_counts = len(dataset) - train_counts\n",
    "\n",
    "print(train_counts, val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_counts, val_counts])\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Now we can create a model and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(2222, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 400\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 100\n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "if os.path.exists('./models/state_dict.pt'):\n",
    "    print(\"Loading existing model...\")\n",
    "    # model.load_state_dict(torch.load('./models/state_dict.pt'))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    for inputs, labels in train_dl:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float().squeeze())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "    val_h = model.init_hidden(BATCH_SIZE)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in val_dl:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)[]\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float().squeeze())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_dl.dataset)\n",
    "    epoch_val_acc = val_acc/len(val_dl.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}/{epochs}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), './models/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference On Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "inference_model.load_state_dict(torch.load('./models/state_dict.pt'))\n",
    "inference_model.eval()\n",
    "inference_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text):\n",
    "    text_tensor = torch.tensor([START_IDX] + text_pipeline(text) + [END_IDX], dtype=torch.int32).to(device).unsqueeze(0)\n",
    "    batch_size = 1\n",
    "    h = model.init_hidden(batch_size)\n",
    "    h = tuple([x.data for x in h])\n",
    "    output, h = model(text_tensor, h)\n",
    "    prediction = output.item()\n",
    "    return 1 if prediction > 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predictions = [] # {id,prediction}\n",
    "\n",
    "for id_, text in tqdm(list(pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), index_col='id').itertuples(index=True, name=None))):\n",
    "    prediction = inference(model, text)\n",
    "    predictions.append({'id': id_, 'sentiment': prediction})\n",
    "\n",
    "    \n",
    "preds = pd.DataFrame(predictions)\n",
    "preds.to_csv('submission.csv', index=False)\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78495c176dcf7610bcdecaf3a7823a54fa8238d5432e6157930bbc78e0bf3e46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
