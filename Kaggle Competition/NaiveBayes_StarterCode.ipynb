{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Starter Code\n",
    "Use this code as a template, starting place, or inspiration... whatever helps you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This starter code requires the use of certain packages. You can install them via the command: `pip install seaborn nltk  beautifulsoup4 pandas sklearn tqdm numpy`\n",
    "\n",
    "If you are using Google Colab, you do not need to do this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Standard Library Packages\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Import Plotting Packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Text Processing Packages\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import Data Loading / ML Packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize configs / constants\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "nltk.download('stopwords')\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "\n",
    "# Disable warnings for Beautiful Soup Package\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "Visit [https://www.kaggle.com/competitions/osuaiclub-fall2022-sentiment-analysis/data](https://www.kaggle.com/competitions/osuaiclub-fall2022-sentiment-analysis/data) to download the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "We will be using the `pandas` package to load in our data. All the data is conveniently stored in a `.csv` file which is really easy to construct a `pandas` dataframe out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(DATA_DIR, 'train.csv')):\n",
    "    train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), index_col='id')\n",
    "else:\n",
    "    train_df = pd.read_csv(\"https://raw.githubusercontent.com/OSU-AIClub/Fall-2022/main/Kaggle%20Competition/data/train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tokenizer\n",
    "Now that we have our data, we need to tokenize our input for the classifier to be able to process our input. Here, we are using sipmly just regex expressions, removing stop words, and normalization to tokenize and process our text. Consider adding different strategies such as stemming, lemmatization, parts of speech tagging, and named entity recognition to improve the accuracy further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "  \n",
    "  def clean(self, text):\n",
    "      # Remove HTML Tags from input\n",
    "      no_html = BeautifulSoup(text).get_text()\n",
    "      \n",
    "      # Replace anything that isn't a letter with a space\n",
    "      clean = re.sub(\"[^a-z\\s]+\", \" \", no_html, flags=re.IGNORECASE)\n",
    "      \n",
    "      # Replace all whitespace with a single space\n",
    "      return re.sub(\"(\\s+)\", \" \", clean)\n",
    "\n",
    " \n",
    "  def tokenize(self, text):\n",
    "      # Clean input text\n",
    "      clean = self.clean(text).lower()\n",
    "      \n",
    "      # Use predefined list NLTK's English stopwords\n",
    "      stopwords_en = stopwords.words(\"english\")\n",
    "      \n",
    "      # Return the list of tokens (i.e. tokenized text)\n",
    "      return [w for w in re.split(\"\\W+\", clean) if not w in stopwords_en]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definte Naive-Bayes Classifier\n",
    "Next, we need to define our Naive-Bayes Classifier. There are many prebuilt classifiers, but we will be creating our own \"from scratch\". Play around and try different ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "  \n",
    "    def __init__(self, classes, tokenizer):\n",
    "      self.tokenizer = tokenizer\n",
    "      self.classes = classes\n",
    "      \n",
    "    def group_by_class(self, X, y):\n",
    "      data = dict()\n",
    "      for c in self.classes:\n",
    "        data[c] = X[np.where(y == c)]\n",
    "      return data\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "          self.n_class_items[c] = len(data)\n",
    "          self.log_class_priors[c] = math.log(self.n_class_items[c] / n)\n",
    "          self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "          for text in tqdm(data):\n",
    "            counts = Counter(self.tokenizer.tokenize(text))\n",
    "            for word, count in counts.items():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "                \n",
    "        return self\n",
    "      \n",
    "    def laplace_smoothing(self, word, text_class):\n",
    "      num = self.word_counts[text_class][word] + 1\n",
    "      denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "      return math.log(num / denom)\n",
    "      \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in tqdm(X):\n",
    "          \n",
    "          class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "          words = set(self.tokenizer.tokenize(text))\n",
    "          for word in words:\n",
    "              if word not in self.vocab: continue\n",
    "\n",
    "              for c in self.classes:\n",
    "                \n",
    "                log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                class_scores[c] += log_w_given_c\n",
    "                \n",
    "          result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train and Validation Partitions\n",
    "Now that we have defined the tokenizer and naive-bayes classifier, we now need to load the training data and split it into separate partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of input and output values for each training sample\n",
    "X = train_df['review'].values\n",
    "y = train_df['sentiment'].values\n",
    "  \n",
    "# Randomly split into a training and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Naive-Bayes Classifier\n",
    "Now that we have prepared the training data partition, we can instantiate and fit the Naive-Bayes Classifier on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model and Fit it to the Training Data\n",
    "MNB = MultinomialNaiveBayes(\n",
    "    classes=np.unique(y), \n",
    "    tokenizer=Tokenizer()\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Performance\n",
    "Now that we have fit our Naive-Bayes Classifier, we can predict the sentiment on the validation dataset to get an idea of how accurate our model is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sentiment on the validation dataset split\n",
    "y_hat = MNB.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy on the validation split\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More performance metrics\n",
    "print(classification_report(y_val, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prdouce Confusion Matrix to  Visualize Results\n",
    "To visualize our True Positive, True Negative, False Positive, and False Negative predictions, we will plot a confusion matrix using `seaborn` and `pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce confusion matrix of true positives, true negatives, false postives, and false negatives\n",
    "cnf_matrix = confusion_matrix(y_val, y_hat)\n",
    "\n",
    "# Create Plot\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual sentiment')\n",
    "plt.xlabel('Predicted sentiment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Test Dataset\n",
    " Our model is now trained, and we have a good accuracy of `85%`! This means we can now predict the sentiment of each sample in the test dataset and save it to a `.csv` file so we can submit it to the Kaggle competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test dataset from file or internet\n",
    "if os.path.exists(os.path.join(DATA_DIR, 'test.csv')):\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), index_col='id')\n",
    "else:\n",
    "    test_df = pd.read_csv(\"https://raw.githubusercontent.com/OSU-AIClub/Fall-2022/main/Kaggle%20Competition/data/test.csv\")\n",
    "\n",
    "# Get input values and predict sentiment\n",
    "test_inputs = test_df['review'].values\n",
    "predictions = MNB.predict(test_inputs)\n",
    "submission = {'sentiment': predictions}\n",
    "\n",
    "# Convert predictions to a Pandas Dataframe   \n",
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.index.name = 'id'\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to a CSV file for submission\n",
    "submission_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Predictions\n",
    "And that is it! We now have a `.csv` file containing sentiment predictions which we can now upload to get scored! Go to https://www.kaggle.com/competitions/osuaiclub-fall2022-sentiment-analysis/ and submit your prediction file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fc8f51b3e16566cc87b17e2b645c7bac967f770095d25cf6c3af0b0fb8cea7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
